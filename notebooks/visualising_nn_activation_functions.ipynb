{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vshlemon/colabs/blob/main/notebooks/visualising_nn_activation_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM4SCdWivwJV"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/vshlemon/colabs.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author: @kmario23\n",
        "A simple script to visualize common activation functions (i.e. non-linearities) used in Neural Networks.\n",
        "Gives good intuition for how the values of logits would behave after a particular non-linearity is applied.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "def plot_activation_func(inp=1.0, activ_func=lambda x: x, name='non-linearity'):\n",
        "    \"\"\"\n",
        "    Function to perform 3D plot of a given activation function\n",
        "    \"\"\"\n",
        "    ws = np.arange(-5, 5, 0.2)\n",
        "    bs = np.arange(-5, 5, 0.2)\n",
        "\n",
        "    X, Y = np.meshgrid(ws, bs)\n",
        "    lst = [activ_func(torch.tensor(w * inp + b)) for w, b in zip(X.flatten(), Y.flatten())]\n",
        "    os = np.array(lst)\n",
        "    Z = os.reshape(X.shape)\n",
        "\n",
        "    # plot figure\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle('3D plot of ' + name + ' non-linearity', fontsize=16, backgroundcolor='C5', color='C9')\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.set_xlabel('X axis')\n",
        "    ax.set_ylabel('Y axis')\n",
        "    ax.set_zlabel('Z axis')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1, color='green', antialiased=True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # plot sigmoid\n",
        "    plot_activation_func(1, torch.sigmoid, 'Sigmoid')\n",
        "\n",
        "    # plot log-sigmoid\n",
        "    plot_activation_func(1, F.logsigmoid, 'Log-Sigmoid')\n",
        "\n",
        "    # plot ReLU\n",
        "    plot_activation_func(1, torch.relu, 'ReLU')\n",
        "\n",
        "    # plot RReLU\n",
        "    plot_activation_func(1, F.rrelu, 'Randomized ReLU')\n",
        "\n",
        "    # plot Leaky ReLU\n",
        "    plot_activation_func(1, F.leaky_relu, 'Leaky ReLU')\n",
        "\n",
        "    # plot ELU\n",
        "    plot_activation_func(1, F.elu, 'ELU')\n",
        "\n",
        "    # plot SELU\n",
        "    plot_activation_func(1, F.selu, 'SELU')\n",
        "\n",
        "    # plot hardshrink\n",
        "    plot_activation_func(1, F.hardshrink, 'HardShrink')\n",
        "\n",
        "    # plot tanhshrink\n",
        "    plot_activation_func(1, F.tanhshrink, 'TanHShrink')\n",
        "\n",
        "    # plot softsign\n",
        "    plot_activation_func(1, F.softsign, 'SoftSign')\n",
        "\n",
        "    # plot softplus\n",
        "    plot_activation_func(1, F.softplus, 'SoftPlus')\n",
        "\n",
        "    # plot softmin\n",
        "    plot_activation_func(1, F.softmin, 'SoftMin')\n",
        "\n",
        "    # plot softmax\n",
        "    plot_activation_func(1, F.softmax, 'SoftMax')\n",
        "\n",
        "    # plot log-softmax\n",
        "    plot_activation_func(1, F.log_softmax, 'LogSoftmax')\n",
        "\n",
        "    # plot gumbel-softmax\n",
        "    plot_activation_func(1, F.gumbel_softmax, 'Gumbel-SoftMax')\n",
        "\n",
        "    # plot tanh\n",
        "    plot_activation_func(1, torch.tanh, 'TanH')\n",
        "\n",
        "    # plot hardtanh\n",
        "    plot_activation_func(1, F.hardtanh, 'Hard TanH')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "18NLTitS-vmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_activation_func(1, torch.sigmoid, 'Sigmoid')"
      ],
      "metadata": {
        "id": "RpraZ5Ubj8rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMkTI8AgkKp3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}