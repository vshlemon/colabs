{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vshlemon/colabs/blob/main/notebooks/karpathy_nns/chapter_2_makemore_i.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdRQp-jvxDxW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/vshlemon/colabs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration"
      ],
      "metadata": {
        "id": "fRhP_Foqz6YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/colabs/notebooks/karpathy_nns/data'\n",
        "words = open(f'{data_dir}/names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "CO9VDMGCy53N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words[:5]"
      ],
      "metadata": {
        "id": "N2nE6d-1zBPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical information in words**\n",
        "\n",
        "Karpathy mentions there is already a lot of statistical data to learn from in a word. For example in `isabella` we can look at which letter follows the preceding context and use that as data to train a model to learn statistical associations between:\n",
        "  - i -> s\n",
        "  - is -> a\n",
        "  - isa -> b\n",
        "  - ...\n",
        "  - isabella -> [end]\n",
        "\n",
        "So even just one name has a lot of training data."
      ],
      "metadata": {
        "id": "TyzhGq2Y5BT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"\n",
        "  Number of names: {len(words)},\n",
        "  Smallest name size: {min([len(w) for w in words])} characters long,\n",
        "  Largest name size: {max([len(w) for w in words])} characters long\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "4eVpTvnk4_ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Models\n",
        "\n",
        "These are the simplest language models that can be learnt - they are simply models of the relationship between coupled characters. For e.g the bigrams in `Happy` are:\n",
        "  - (H, a)\n",
        "  - (a, p)\n",
        "  - (p, p)\n",
        "  - (p, y)\n",
        "\n",
        "And we train a model over these bigrams to learn statistical associations of which character is likely to follow given the preceding one. Thus, we are conditioning on a history/context of a single character and outputting a single succeeding character."
      ],
      "metadata": {
        "id": "37Wty-br7FWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the word and its succeeding word (by using the words original characters aligned\n",
        "# with its characters offset by 1 - when the zip items are no longer aligned i.e. there is\n",
        "# nothing in one of them, then the loop halts, that is why the last character has no follow\n",
        "# up since there is nothing in the offset list to match with it)\n",
        "\n",
        "print(\"Current character -> Succeeding character\\n\")\n",
        "for w in words[:5]:\n",
        "  print(f\"Name: {w}\")\n",
        "  for ch_curr, ch_next in zip(w, w[1:]):\n",
        "    print(f\"\"\"{ch_curr} -> {ch_next}\"\"\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "00T10Ntw7C12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start and End Tokens**\n",
        "\n",
        "Since we also want to learn patterns of the start of words so that when we have a blank space we have a statistical association of what might follow it to start a new word (and this also extends to learning associations for blank spaces preceded by characters eg. `hello _`), we add a start token `<S>`.\n",
        "\n",
        "And to learn patterns for when to end words we add an end token `<E>` so we develop some statistical associations of which preceding sequence of characters might result in the termination of the word."
      ],
      "metadata": {
        "id": "jVc6b_-_9eC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Current character -> Succeeding character (inc. `<S>` & `<E>` tokens) \\n\")\n",
        "for w in words[:5]:\n",
        "  print(f\"Name: {w}\")\n",
        "  chs = ['<S>'] + list(w) + ['<E>']\n",
        "  for ch_curr, ch_next in zip(chs, chs[1:]):\n",
        "    print(f\"\"\"{ch_curr} -> {ch_next}\"\"\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "CsNgYkbj6RLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collecting Statistics of Bigram Associations**\n",
        "\n",
        "We can collect some statistics to understand better which letters most often precede or succeed others"
      ],
      "metadata": {
        "id": "Ynpd5XfmAFy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = {}\n",
        "for w in words[:5]:\n",
        "  chs = ['<S>'] + list(w) + ['<E>']\n",
        "  for ch_curr, ch_next in zip(chs, chs[1:]):\n",
        "    bigram = (ch_curr, ch_next)\n",
        "    bigrams[bigram] = bigrams.get(bigram, 0) + 1 # if the bigram has not been seen yet set it's default count to 0 & add 1, else get its current count and add 1\n",
        "bigrams"
      ],
      "metadata": {
        "id": "cTNoTmed-f1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For eg. we can see from the above that `<E>` i.e. the end of the name is most often preceded by `a` that is to say of the 5 names we collected statistics for, they all end in `a`. Now we will collect statistics for all of the names' bigrams."
      ],
      "metadata": {
        "id": "BnZ-sbvZA-s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = {}\n",
        "for w in words:\n",
        "  chs = ['<S>'] + list(w) + ['<E>']\n",
        "  for ch_curr, ch_next in zip(chs, chs[1:]):\n",
        "    bigram = (ch_curr, ch_next)\n",
        "    bigrams[bigram] = bigrams.get(bigram, 0) + 1"
      ],
      "metadata": {
        "id": "NfUCFSawA557"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also sort the bigrams my most frequent to least to get a bit more of a feel for them\n",
        "# we do this by getting the items from the dictionary, and then using the value as the key to\n",
        "# sort on but we use it's negative to sort as sorted by default sorts in ascending order & so\n",
        "# the larger the negative number the earlier it will be in the order\n",
        "sorted(bigrams.items(), key=lambda kv: -kv[1])[:5] # value is second element of key value (kv) pairs, we print just a few as there's a lot of possible bigrams"
      ],
      "metadata": {
        "id": "lJ4JSk3dCR4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Stretch Question__:\n",
        "- How many possible bigrams are there with 26 alphabetic letters?\n",
        "  - Since order matters here we are looking for permutations not combinations, the latter are distinct sets of numbers. Thus, since we are looking for groups of size 2 we do:\n",
        "    - `26! * (1/(26-2)!)` which is effectively `26 * 25` i.e. **650** unique bigram pairings (this allows for things like `(a,b)` and `(b,a)` - if we wanted combinations where we only cared about the members not their order then we would further divide by `2!` to get _350_ combinations, since this eliminates any duplicative counts of sequences with the same members but in different orderings)"
      ],
      "metadata": {
        "id": "rQBhYoNBDE6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Structuring as a Matrix & Visualising**\n",
        "It is preferable to have our bigram counts in a matrix shape of rows and columns that we visualise so we can more easily identify the statistical associations. We will do this using PyTorch tensors as the library has good matrix methods and can be extended further on for our deep-learning requirements"
      ],
      "metadata": {
        "id": "BmelTxOhFADX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "QTUYFHbjBVKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAP = torch.zeros((28, 28), dtype=torch.int32) # an zero initialised 28*28 size array with int32 precision (reason for 28*28 is 26 alphabets and `<S>` & `<E>` tokens)"
      ],
      "metadata": {
        "id": "lUDNxGUZBW4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need a way to map from the characters to integers (so we can use them as indexes in our mapping matrix)\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "stoi['<S>'] = 26\n",
        "stoi['<E>'] = 27\n",
        "#stoi"
      ],
      "metadata": {
        "id": "-y2SUZXCHCEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "  chs = ['<S>'] + list(w) + ['<E>']\n",
        "  for ch_curr, ch_next in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch_curr]\n",
        "    ix2 = stoi[ch_next]\n",
        "    MAP[ix1, ix2] += 1 # adding a count of 1 every time we see a particular bigram pairing (remember MAP's values start at 0)\n",
        "#MAP"
      ],
      "metadata": {
        "id": "QCgUZszZH_bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a reverse mapping from integers to string so we can use the actual characters when needed\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "#itos"
      ],
      "metadata": {
        "id": "m4VN6yXDIuTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising it\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "'''\n",
        "Creates a bluescale image colormap of MAP (this gives shading to the cells\n",
        "dependent on the value within the matrix at that cell - higher the value\n",
        "darker the cell colour)\n",
        "\n",
        "Then iterates through all possible bigram index combinations (nested loops\n",
        "of 28 size each) and gets the characters at that location, and on the plot\n",
        "places those characters as text at the index location, as well places the\n",
        "count from the matrix collected above at the same index location\n",
        "'''\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(MAP, cmap='Blues')\n",
        "for i in range(28):\n",
        "    for j in range(28):\n",
        "        chstr = itos[i] + itos[j]\n",
        "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
        "        plt.text(j, i, MAP[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "fNDUMa45IkZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see above that there is a redundant row at the very bottom since there is never another character immediately succeeding an end `<E>` character of a word, since it is always the last token of the word that indicates nothing else should come after.\n",
        "\n",
        "You can also see there is a redundant column since the `<S>` token indicates a word about to begin so there is never anything before it, thus it can never appear at the second place in a bigram."
      ],
      "metadata": {
        "id": "fkz2GdK_K7DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a single special token**\n",
        "\n",
        "We are going to utilise only a single special token to indicate both start and end of the word, and we will place it at the 0th (i.e. first) index of our alphabet - just out of a preference in visualising.\n",
        "\n",
        "We are going to use `.` as our special token, this means this character will now signify the start and end of a word, so it will basically serve as word boundary.\n",
        "\n",
        "This works since `<S>` is only used in one context of the bigram pairing, at the start, and `<E>` is only used in one context, at the end. And so effectively our new character `.` will serve as an `<S>` when learning associations for characters preceding the starting character of words and as an `<E>` when learning associations for characters succeeding the final character of words.\n",
        "\n",
        "The statistical associations learnt will be along the lines of:\n",
        "- Learn for a `.` to succeed words according to the statistics of it's pairing with the characters they end on - removing the redundancy of the `<S>` column\n",
        "- Learn for `.` to preceed words according to the statistics of it's pairing with the characters they begin on - removing the redundancy of the `<E>` column"
      ],
      "metadata": {
        "id": "xUnV8gfaLuOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAP = torch.zeros((27, 27), dtype=torch.int32) # 27*27 is 26 alphabets and an `.` token"
      ],
      "metadata": {
        "id": "8Ri-PtYnIqgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)} # offset by 1, so `.` can take 0th index\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "0r9kqkK9Nx0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.'] # swapping out old start/end tokens for `.`\n",
        "  for ch_curr, ch_next in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch_curr]\n",
        "    ix2 = stoi[ch_next]\n",
        "    MAP[ix1, ix2] += 1"
      ],
      "metadata": {
        "id": "owOo8icON5d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(MAP, cmap='Blues')\n",
        "for i in range(27): # now only 27 rows\n",
        "    for j in range(27): # now only 27 cols\n",
        "        chstr = itos[i] + itos[j]\n",
        "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
        "        plt.text(j, i, MAP[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "u7bUWFm3OD-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is now much easier to interpret, the first row is the count of all starting letters of words (eg. `a` is the most popular starting letter of the names in our dataset) & the first column is the count of all ending letters of names (where `n` is the most common ending)"
      ],
      "metadata": {
        "id": "P_sAXQceOZfr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYJxxTa1O2H4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
