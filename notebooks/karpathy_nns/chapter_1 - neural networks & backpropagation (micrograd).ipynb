{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVHf0r2GKHWGq7JCoPTZzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vshlemon/colabs/blob/main/notebooks/karpathy_nns/chapter_1%20-%20neural%20networks%20%26%20backpropagation%20(micrograd).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF0zA76UWVEN"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/vshlemon/colabs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *The spelled out intro to neural networks and backpropagation: building micrograd*\n",
        "\n",
        "- [Youtube video](https://www.youtube.com/watch?v=VMj-3S1tku0&t=7386s)\n",
        "  - [Jupyter notebooks from video](https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd)\n",
        "  - [Exercises for after video](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbW51Y3ZCcWdoamQ3S00wY1pHLWNVTWRtSmRWQXxBQ3Jtc0tsQ3BlSlZIY1ptdWRuWEFJaDc0Wnk5YlI3ekhPRTR6OUpIdmxOLVREaDFucUtyUHdrU2w3UHFCdTNkT0pQTkV6RWNVcU1KZnNOVzBmUnFER3Y4SElzX0tYb1lXRl80aXlaN3N1SmFITERDRjNoS1hjYw&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN%3Fusp%3Dsharing&v=VMj-3S1tku0)\n",
        "- [Maths solver](https://www.google.com/search?q=step+by+step+math+solver)\n",
        "  - try [Symbolab](https://www.symbolab.com) or [Wolfram Alpha](https://www.wolframalpha.com/input?i=derivative) if above doesn't work"
      ],
      "metadata": {
        "id": "vOYbREZprPlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "A3exyQLEt-34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Understanding derivatives of functions\n",
        "\n",
        "Derivatives tell us the rate of change of a function at a particular point within the domain of it's input values. They form the basis of backpropagation, the technique by which neural networks learn to reduce error and make more accurate predictions/generations."
      ],
      "metadata": {
        "id": "xXEEyflJsVKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 3*x**2 - 4*x + 5"
      ],
      "metadata": {
        "id": "5NlYz0lLrNzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(3.0)"
      ],
      "metadata": {
        "id": "L8efx4k50UBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function above has only a single variable on which it operates, we can plot how the function behaves for a subset of input values within its domain"
      ],
      "metadata": {
        "id": "mqAqFxj8sqxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = np.arange(-5, 5, 0.25) # range from -5 to 5, with 0.25 step intervals\n",
        "ys = f(xs) # computing the function on every input value\n",
        "plt.plot(xs, ys)\n",
        "plt.xlabel('input values')\n",
        "plt.ylabel('function response/output')"
      ],
      "metadata": {
        "id": "MekEpZuWt8eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating derivatives numerically**\n",
        "\n",
        "We can calculate the derivative, i.e. the rate of change, by estimating the slope of the function at a particular point.\n",
        "\n",
        "We can get this estimation by adding a miniscule amount to the input value and calculating the function at this slightly changed location. We can then calculate the difference between the function at the location we intend to estimate for and this slightly changed location to get the change in the function value, and we can then get the rate of change by normalising (dividing out) by the miniscule amount we changed it by e.g:\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h}$$\n",
        "\n",
        "Here, $h$ is the amount we add, which must be miniscule so we can get an approximation of the rate of change around $x$ which is the location we care to evaluate the derivative for, by subtracting the function at $x$ from it offset by $h$ we get a good local approximation - we then divide by $h$ so that we don't simply get the change in the function value (y-axis only), but we get the rate of change i.e. slope of it (factoring in y-axis change over the x-axis change $h$).\n",
        "\n",
        "We can write a function for this below. *Note that as $h$ gets smaller it becomes harder for programming languages like python to handle as they can not manage with that level of precision in representing a number, there are tricks to deal with this such as using the logarithm of a decimal number etc.*"
      ],
      "metadata": {
        "id": "4_p5Pm2Fu3rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_in_function(f, x, h):\n",
        "  return (f(x + h) - f(x))\n",
        "\n",
        "def numerical_derivative(f, x, h):\n",
        "  return (\n",
        "      change_in_function(f, x, h) / h\n",
        "  )"
      ],
      "metadata": {
        "id": "eN0P8mZpuFOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 2/3 # 0.66666666\n",
        "h = 0.000001\n",
        "print(f\"\"\"\n",
        "  The function changes by {change_in_function(f, x)}, over a distance of {h}\n",
        "  giving a derivative/slope of: {numerical_derivative(f, x)}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "x5YzVy7zx5xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can see that at the point where $x=0.66666$ it's rate of change is very little but is moving in the positive direction (because $f(x + h)$ is greater than $f(x)$) - and as you can see in the graph it begins to bottom out/slow down around that area.\n",
        "\n",
        "We can try to plot this"
      ],
      "metadata": {
        "id": "ZyLuLK_A07Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N86Nc9UjzusF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}