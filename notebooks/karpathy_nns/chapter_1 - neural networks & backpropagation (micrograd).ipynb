{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpkpsVzp5nRUxlRgMMpjEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vshlemon/colabs/blob/main/notebooks/karpathy_nns/chapter_1%20-%20neural%20networks%20%26%20backpropagation%20(micrograd).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF0zA76UWVEN"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/vshlemon/colabs.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *The spelled out intro to neural networks and backpropagation: building micrograd*\n",
        "\n",
        "- [Youtube video](https://www.youtube.com/watch?v=VMj-3S1tku0&t=7386s)\n",
        "  - [Jupyter notebooks from video](https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd)\n",
        "  - [Exercises for after video](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbW51Y3ZCcWdoamQ3S00wY1pHLWNVTWRtSmRWQXxBQ3Jtc0tsQ3BlSlZIY1ptdWRuWEFJaDc0Wnk5YlI3ekhPRTR6OUpIdmxOLVREaDFucUtyUHdrU2w3UHFCdTNkT0pQTkV6RWNVcU1KZnNOVzBmUnFER3Y4SElzX0tYb1lXRl80aXlaN3N1SmFITERDRjNoS1hjYw&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN%3Fusp%3Dsharing&v=VMj-3S1tku0)\n",
        "- [Maths solver](https://www.google.com/search?q=step+by+step+math+solver)\n",
        "  - try [Symbolab](https://www.symbolab.com) or [Wolfram Alpha](https://www.wolframalpha.com/input?i=derivative) if above doesn't work"
      ],
      "metadata": {
        "id": "vOYbREZprPlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "A3exyQLEt-34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Understanding derivatives of functions\n",
        "\n",
        "Derivatives tell us the rate of change of a function at a particular point within the domain of it's input values. They form the basis of backpropagation, the technique by which neural networks learn to reduce error and make more accurate predictions/generations."
      ],
      "metadata": {
        "id": "xXEEyflJsVKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return 3*x**2 - 4*x + 5"
      ],
      "metadata": {
        "id": "5NlYz0lLrNzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(3.0)"
      ],
      "metadata": {
        "id": "L8efx4k50UBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function above has only a single variable on which it operates, we can plot how the function behaves for a subset of input values within its domain"
      ],
      "metadata": {
        "id": "mqAqFxj8sqxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = np.arange(-5, 5, 0.25) # range from -5 to 5, with 0.25 step intervals\n",
        "ys = f(xs) # computing the function on every input value\n",
        "plt.plot(xs, ys)\n",
        "plt.xlabel('input values')\n",
        "plt.ylabel('function response/output')"
      ],
      "metadata": {
        "id": "MekEpZuWt8eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating derivatives numerically**\n",
        "\n",
        "We can calculate the derivative, i.e. the rate of change, by estimating the slope of the function at a particular point.\n",
        "\n",
        "We can get this estimation by adding a miniscule amount to the input value and calculating the function at this slightly changed location. We can then calculate the difference between the function at the location we intend to estimate for and this slightly changed location to get the change in the function value, and we can then get the rate of change by normalising (dividing out) by the miniscule amount we changed it by e.g:\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h}$$\n",
        "\n",
        "Here, $h$ is the amount we add, which must be miniscule so we can get an approximation of the rate of change around $x$ which is the location we care to evaluate the derivative for, by subtracting the function at $x$ from it offset by $h$ we get a good local approximation - we then divide by $h$ so that we don't simply get the change in the function value (y-axis only), but we get the rate of change i.e. slope of it (factoring in y-axis change over the x-axis change $h$).\n",
        "\n",
        "We can write a function for this below. *Note that as $h$ gets smaller it becomes harder for programming languages like python to handle as they can not manage with that level of precision in representing a number, there are tricks to deal with this such as using the logarithm of a decimal number etc.*"
      ],
      "metadata": {
        "id": "4_p5Pm2Fu3rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_in_function(f, x, h):\n",
        "  return (f(x + h) - f(x))\n",
        "\n",
        "def numerical_derivative(f, x, h):\n",
        "  return (\n",
        "      change_in_function(f, x, h) / h\n",
        "  )"
      ],
      "metadata": {
        "id": "eN0P8mZpuFOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 2/3 # 0.66666666\n",
        "h = 0.000001\n",
        "print(f\"\"\"\n",
        "  The function changes by {change_in_function(f, x)}, over a distance of {h}\n",
        "  giving a derivative/slope of: {numerical_derivative(f, x)}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "x5YzVy7zx5xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can see that at the point where $x=0.66666$ it's rate of change is very little but is moving in the positive direction (because $f(x + h)$ is greater than $f(x)$) - and as you can see in the graph it begins to bottom out/slow down around that area.\n",
        "\n",
        "We can try to plot this"
      ],
      "metadata": {
        "id": "ZyLuLK_A07Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_with_tangent_and_change_lines(\n",
        "    f,\n",
        "    xs,\n",
        "    ys,\n",
        "    h,\n",
        "    x_value_to_calculate_derivative_at\n",
        "):\n",
        "  plt.plot(xs, ys)\n",
        "  plt.xlabel('input value')\n",
        "  plt.ylabel('function response')\n",
        "\n",
        "  # Plotting tangent line\n",
        "  # y = m*(x - x1) + y1\n",
        "  def tangent_line(xrange, x1, y1):\n",
        "    '''\n",
        "    Produces the y-values of the tangent line by:\n",
        "\n",
        "    Derivative:\n",
        "      It gets the derivative of the number x1,\n",
        "\n",
        "    Xrange offsets:\n",
        "      It then gets the offsets of all numbers\n",
        "      in the range of x we are plotting for\n",
        "      from x1, so if x1 is 5 and we are plotting\n",
        "      for range x1-1 to x1+1, it will go from 4:6\n",
        "      and subtract 5 from every element in there,\n",
        "      where num elements are determined by the 3rd\n",
        "      arg to np.linspace. This will look like, for\n",
        "      eg. [-1, -0.8 ... 0 ... 0.2, 0.4 ... 1] etc.\n",
        "\n",
        "      It then multiplies the derivative by the x-range\n",
        "      offsets, which intuitively gives us the line hanging\n",
        "      in mid-air (i.e. without rooting at the right y-axis\n",
        "      location) since the derivative gives us the slope &\n",
        "      we are just wanting to draw out the same slope line\n",
        "      across a range of x-values and so we just multiply their\n",
        "      value with the slope to get one straight line (treating\n",
        "      it as if that were there slope line almost) ... & then\n",
        "      finally we add the actual function output for the x value\n",
        "      we got the derivative of in order to peg the hanging line\n",
        "      to the right place i.e. touching at the y-tangent point.\n",
        "\n",
        "      You can also just think of it as a re-arrangement of the\n",
        "      derivative/slope formula:\n",
        "        (y - y1)\n",
        "        -------- = m\n",
        "        (x - x1)\n",
        "      where m is the slope, that can be re-arranged to get the\n",
        "      y point & if we compute it across a range of x-values whilst\n",
        "      holding everything the same we get the slope line stretched\n",
        "      across those x-range values\n",
        "    '''\n",
        "    return (\n",
        "        numerical_derivative(f, x1, h)\n",
        "        * (xrange - x1)\n",
        "        + y1\n",
        "    )\n",
        "\n",
        "  xvda = x_value_to_calculate_derivative_at\n",
        "  # Define x data range for tangent line\n",
        "  tangent_xrange = np.linspace(\n",
        "      xvda - (1+h),\n",
        "      xvda + (1+h),\n",
        "      10\n",
        "  )\n",
        "  plt.plot(\n",
        "      tangent_xrange, # x-axis of tangent line\n",
        "      tangent_line(\n",
        "          tangent_xrange,\n",
        "          xvda,\n",
        "          f(xvda)\n",
        "      ), # y-axis of tangent line\n",
        "      'C1--',\n",
        "      linewidth=2,\n",
        "      label='tangent line'\n",
        "  )\n",
        "\n",
        "  # Plotting the change in x and y for the value to compute derivative of\n",
        "  # Change in x (likely invisible due to miniscule size of h)\n",
        "  plt.plot(\n",
        "      [\n",
        "          xvda,\n",
        "          xvda + h\n",
        "      ],\n",
        "      [f(xvda), f(xvda)],\n",
        "      'C8--',\n",
        "      linewidth=2,\n",
        "      label='change in x line'\n",
        "  )\n",
        "\n",
        "  # Change in y (likely invisible due to miniscule size of h)\n",
        "  plt.arrow(\n",
        "      x=xvda,\n",
        "      y=f(xvda),\n",
        "      dx=0, # dx\n",
        "      dy=(f(xvda+h)-f(xvda)), # dy\n",
        "      color='C4',\n",
        "      linestyle='-',\n",
        "      linewidth=2,\n",
        "      head_width=(0.1+h/2),\n",
        "      label='change in y line'\n",
        "  )\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "N86Nc9UjzusF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = 1\n",
        "xs = np.arange(-5, 5, 0.25)\n",
        "ys = f(xs)\n",
        "\n",
        "x_value_to_calculate_derivative_at = -2\n",
        "\n",
        "plot_with_tangent_and_change_lines(\n",
        "    f, xs, ys, h, x_value_to_calculate_derivative_at\n",
        ")"
      ],
      "metadata": {
        "id": "FxBIvJom3LI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0k3MgXfG3ONC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}